<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My GitHub Page</title>
</head>
<body>
    <h1>Welcome to My GitHub Page</h1>
    <p>This is my personal website hosted on GitHub Pages!</p>

    <h2>Automated Cross-Validation for LWR and Random Forest</h2>

    <p>My code displays a class that implements a Locally Weighted Regression (LWR), a regression technique that emphasizes local data points when making predictions. It also offers the option to compare this method with Random Forest by switching the model type.</p>

    <p>The cross_validate function in the class incorporates all the other functions from the class for optimal automation. It brings together the fit, predict, mse, and r2_score functions to automate the K-fold cross-validation for Locally Weighted Regression (LWR) and Random Forest.</p>

    <h3>Kernel Functions</h3>
    <p>Before we get into the functions within the class, there are four kernel functions outside of the class that help create the weights for LWR:</p>
    <ul>
        <li>Gaussian</li>
        <li>Tricubic</li>
        <li>Epanechnikov</li>
        <li>Quartic</li>
    </ul>
    <p>The different kernel functions allow the user to test and choose the most accurate one.</p>


    <h3>PatricksModel Class Constructor (<code>__init__</code>)</h3>
    <p>The constructor accepts several parameters:</p>
    <ul>
        <li><b>model type</b>: Either 'lwr' for Locally Weighted Regression or 'rf' for Random Forest.</li>
        <li><b>kernel type</b>: Used for LWR, can be 'tricubic', 'gaussian', etc.</li>
        <li><b>neighborhood fraction (f)</b>: Fraction of the data used to determine nearby points in LWR.</li>
        <li><b>number of iterations (iter)</b>: Number of iterations for the LWR fitting process.</li>
        <li><b>include intercept</b>: Whether to include an intercept in the regression (True or False).</li>
    </ul>
    <p><b>Random Forest Parameters</b>: If the model type is 'rf', the constructor initializes a Random Forest with specific parameters:</p>
    <ul>
        <li><b>n_estimators</b>: The number of trees in the forest.</li>
        <li><b>max_depth</b>: The maximum depth of the tree.</li>
        <li><b>random_state</b>: Controls randomness for reproducibility.</li>
    </ul>
    <p>If no parameters are provided, default values are used for Random Forest.</p>

    <h3>Kernel Selection (<code>_get_kernel</code>)</h3>
    <p>Based on the kernel type specified during initialization, this method returns the appropriate kernel function. The available kernel options include:</p>
    <ul>
        <li><b>Gaussian</b></li>
        <li><b>Tricubic</b></li>
        <li><b>Epanechnikov</b></li>
        <li><b>Quartic</b></li>
    </ul>
    <p>If an unknown kernel type is passed during initialization, the method raises an error, ensuring the user selects a valid kernel function.</p>

    <h3>Weight Calculation (<code>_weight_calculation</code>)</h3>
    <p>This function calculates the weights for each training point relative to the new data point. It computes the Euclidean distance between each training point and the new point, applies the selected kernel function, and scales the distances to generate weights. The number of neighbors considered is determined by the fraction (<code>f</code>) of total training samples.</p>
    
    <h3>Prediction (<code>predict</code>)</h3>
    <p>In LWR, predictions are made by scaling the new input data, computing weights using the training data, and solving for the regression coefficients via weighted least squares. The function iterates through the training data to adjust the weights and recalculates residuals to improve predictions. L2 regularization is used for stability.</p>
    <p>For Random Forest, the <code>predict</code> function of the Random Forest model is used to generate predictions.</p>
    
    <h3>Model Evaluation (<code>mse</code>, <code>r2_score</code>)</h3>
    <p>Two functions are provided to evaluate the performance of the model:</p>
    <ul>
        <li><b><code>mse</code></b>: Calculates the Mean Squared Error (MSE) between true and predicted values.</li>
        <li><b><code>r2_score</code></b>: Computes the R² score, which measures how well the model explains the variance in the data.</li>
    </ul>
    
    <h3>Cross-Validate Function (<code>cross_validate</code>)</h3>
    <p>All of these functions cumulate into the <code>cross_validate</code> function. This function automates the evaluation of either Locally Weighted Regression (LWR) or Random Forest by incorporating several key functions in the class, allowing for efficient model training, prediction, and performance assessment.</p>
    
    <h4>Key Steps:</h4>
    <ul>
        <li><b>K-Fold Cross-Validation:</b> It splits the dataset into <code>n_splits</code> (default 10) folds, using each fold as a test set once, while training on the remaining data.</li>
        <li><b>Model Fitting:</b> Calls <code>fit</code> to train either the LWR or Random Forest model on each fold.</li>
        <li><b>Prediction:</b> Uses <code>predict</code> to generate predictions for the test data.</li>
        <li><b>Performance Metrics:</b> Automatically calculates Mean Squared Error (MSE) and R² score for each fold using <code>mse</code> and <code>r2_score</code>.</li>
        <li><b>Averaging:</b> The function averages the MSE and R² scores across all folds and prints the results.</li>
    </ul>
    
    <h4>Key Benefit:</h4>
    <p>Without <code>cross_validate</code>, you’d need to manually split the data, fit the model, predict, and calculate metrics for each fold. This function automates the entire process, making it fast and convenient to evaluate both models on the same dataset.</p>


    <h3>Python Code for Locally Weighted Regression and Random Forest</h3>

    <pre><code>
    data = pd.read_csv('cars(2).csv') 
    y = data['MPG'].values
    X = data.drop(columns=['MPG']).values
    
    import numpy as np
    from scipy.spatial.distance import cdist
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import KFold
    from sklearn.metrics import mean_squared_error
    from scipy.linalg import solve
    
    # Define kernel functions
    def Gaussian(x):
      return np.where(np.abs(x) > 4, 0, 1/(np.sqrt(2*np.pi)) * np.exp(-1/2 * x**2))
    
    def Tricubic(x):
      return np.where(np.abs(x) > 1, 0, (1 - np.abs(x)**3)**3)
    
    def Epanechnikov(x):
      return np.where(np.abs(x) > 1, 0, 3/4 * (1 - np.abs(x)**2))
    
    def Quartic(x):
      return np.where(np.abs(x) > 1, 0, 15/16 * (1 - np.abs(x)**2)**2)
    
    class PatricksModel:
      def __init__(self, model_type = 'lwr', kernel='tricubic', f=1/3, iter=3, intercept=True, rf_params = None):
        self.model_type = model_type
        self.kernel = kernel
        self.f = f
        self.iter = iter
        self.intercept = intercept
        self.scaler = MinMaxScaler()
    
        if self.model_type == 'rf':
          if rf_params is None:
            rf_params = {'n_estimators': 100, 'max_depth': 10, 'random_state': 123}
          self.rf_model = RandomForestRegressor(**rf_params)
    
      def _get_kernel(self):
        if self.kernel == 'gaussian':
            return Gaussian
        elif self.kernel == 'tricubic':
            return Tricubic
        elif self.kernel == 'epanechnikov':
            return Epanechnikov
        elif self.kernel == 'quartic':
            return Quartic
        else:
            raise ValueError("Unknown kernel type")
    
      def fit(self, X, y):
        if self.model_type == 'lwr':
          self.xtrain_ = self.scaler.fit_transform(X)
          self.ytrain_ = y.reshape(-1, 1)
        elif self.model_type == 'rf':
          self.rf_model.fit(X, y)
    
      def _weight_calculation(self, x, x_new, r):
        distances = np.sqrt(np.sum((x - x_new) ** 2, axis=1))
        h = np.sort(distances)[r]  
    
        kernel_fn = self._get_kernel()
        weights = np.clip(distances / h, 0.0, 1.0)
        return kernel_fn(weights)  
    
      def predict(self, x_new):
        if self.model_type == 'lwr':
    
          x_new_scaled = self.scaler.transform(x_new)
          number_training_samples = len(self.xtrain_)
          number_neighbors = int(np.ceil(self.f * number_training_samples))
          predictions = np.zeros(len(x_new_scaled))
    
          if self.intercept:
              xtrain_1 = np.column_stack([np.ones((len(self.xtrain_), 1)), self.xtrain_])
          else:
              xtrain_1 = self.xtrain_
    
          delta = np.ones(number_training_samples)  
    
          for iteration in range(self.iter):
              for i, new_data_point in enumerate(x_new_scaled):
                  weights = self._weight_calculation(self.xtrain_, new_data_point, number_neighbors)
    
                  W = np.diag(delta).dot(np.diag(weights))
    
                  regularized_matrix = np.transpose(xtrain_1).dot(W).dot(xtrain_1)
                  regularized_matrix += 0.0001 * np.eye(xtrain_1.shape[1])  # L2 regularization
                  weighted_result = np.transpose(xtrain_1).dot(W).dot(self.ytrain_)
                  coefficients = solve(regularized_matrix, weighted_result)
    
                  if self.intercept:
                      new_data_point_with_bias = np.concatenate([[1], new_data_point])
                  else:
                      new_data_point_with_bias = new_data_point
                  predictions[i] = np.dot(new_data_point_with_bias, coefficients)
    
                  
              residuals = self.ytrain_.ravel() - np.dot(xtrain_1, coefficients).ravel()
              s = np.median(np.abs(residuals))
              delta = np.clip(residuals / (6.0 * s), -1, 1)
              delta = (1 - delta ** 2) ** 2
    
          return predictions
    
        elif self.model_type == 'rf':
          return self.rf_model.predict(x_new) #This is the Random Forest predict, not the classes predict
    
      def mse(self, y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)
    
      def r2_score(self, y_true, y_pred):
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        return 1 - (ss_res / ss_tot)
    
      def cross_validate(self, X, y, n_splits=10):
    
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=123)
        mse_scores = []
        r2_scores = []
    
        for train_index, test_index in kf.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
    
            self.fit(X_train, y_train)
            y_pred = self.predict(X_test)
    
            mse = self.mse(y_test, y_pred)
            r2 = self.r2_score(y_test, y_pred)
    
            mse_scores.append(mse)
            r2_scores.append(r2)
    
        model_name = 'Locally Weighted Regression' if self.model_type == 'lwr' else 'Random Forest'
    
        print(f"10-fold Cross-Validated MSE for {model_name}: {np.mean(mse_scores)}")
        print(f"10-fold Cross-Validated R² for {model_name}: {np.mean(r2_scores)}")
    
        return np.mean(mse_scores), np.mean(r2_scores)
    </code></pre>


    


</body>
</html>
