<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patrick Church - Assignment 3</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #e0f7fa;
            margin: 0;
            padding: 0;
        }
        header {
            background-color: #0277bd;
            color: white;
            padding: 20px;
            text-align: center;
        }
        section {
            margin: 20px;
            padding: 20px;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1, h2 {
            color: #01579b;
        }
        p {
            font-size: 1.1em;
            line-height: 1.6;
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-left: 5px solid #0277bd;
            overflow-x: auto;
            border-radius: 4px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        table, th, td {
            border: 1px solid #ccc;
        }
        th, td {
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #0288d1;
            color: white;
        }
        ul {
            color: #0277bd;
        }
    </style>
</head>
<body>

<header>
    <h1>Patrick Church - Assignment 3</h1>
    <p>October 9, 2024</p>
</header>

<section>
    <h2>SCAD Regularization and Variable Selection in PyTorch</h2>
    <p>
        This assignment involved implementing a PyTorch class that applies SCAD (smoothly clipped absolute deviation) regularization and variable selection for linear models. The implementation was based on the following references:
    </p>
    <ul>
        <li><a href="https://andrewcharlesjones.github.io/journal/scad.html" target="_blank">SCAD Method Overview</a></li>
        <li><a href="https://www.jstor.org/stable/27640214?seq=1" target="_blank">JSTOR SCAD Regularization</a></li>
    </ul>
    <p>
        The PyTorch class <code>SCADLinearModel</code> was developed to handle linear regression using SCAD regularization. The code includes a forward pass function, a SCAD penalty computation, and a custom loss function that combines mean squared error (MSE) with the SCAD penalty. Below is an example of the code implementation.
    </p>

    <h3>Class Implementation</h3>
    <pre><code class="language-python">
class SCADLinearModel(nn.Module):
    def __init__(self, input_dim, alpha=1.0, a=3.7, device="cpu"):
        super(SCADLinearModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1).double()
        self.alpha = alpha
        self.a = a
        self.device = device
        self.to(device)

    def scad_penalty(self, beta_hat, lambda_val, a_val):
        abs_beta_hat = torch.abs(beta_hat)
        is_linear = abs_beta_hat <= lambda_val
        is_quadratic = (lambda_val < abs_beta_hat) & (abs_beta_hat <= a_val * lambda_val)
        is_constant = abs_beta_hat > a_val * lambda_val
        linear_part = lambda_val * abs_beta_hat * is_linear
        quadratic_part = (2 * a_val * lambda_val * abs_beta_hat - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic
        constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant
        return linear_part + quadratic_part + constant_part

    def forward(self, x):
        return self.linear(x)

    def compute_loss(self, predictions, targets):
        mse_loss = nn.functional.mse_loss(predictions, targets)
        scad_penalty = self.scad_penalty(self.linear.weight, self.alpha, self.a)
        return mse_loss + scad_penalty.sum()


# Testing on the concrete dataset
x = data.drop(columns = ['strength'])
y = data['strength']

x_train_np = x.to_numpy()
y_train_np = y.to_numpy()

x_train = torch.tensor(x_train_np, dtype=torch.double)
y_train = torch.tensor(y_train_np.reshape(-1, 1), dtype=torch.double)

model = SCADLinearModel(input_dim=x_train.shape[1], alpha=1.0, a=3.5, device="cpu")
    
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
        
    predictions = model(x_train)
        
    loss = model.compute_loss(predictions, y_train)
        
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
        
    if (epoch + 1) % 100 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
    
with torch.no_grad():
    weights = model.linear.weight.cpu().numpy()

feature_names = x.columns

feature_importance = list(zip(feature_names, weights[0]))  

print("\nAll Coefficients (Weights):")
print("--------------------------------")
for feature, weight in feature_importance:
    print(f"{feature:<14} : {weight:.6f}")
print("--------------------------------")

feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)

print("\nFeature Importance (sorted by absolute weight value):")
print("-----------------------------------------------------")
print("| Feature        |    Weight    |   Importance Rank  |")
print("-----------------------------------------------------")
for rank, (feature, weight) in enumerate(feature_importance, 1):
    print(f"| {feature:<14} |  {weight:>9.4f}   |        {rank:<2}          |")
print("-----------------------------------------------------")
    </code></pre>

    <h3>Feature Coefficients</h3>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Weight</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>cement</td>
                <td>0.114060</td>
            </tr>
            <tr>
                <td>slag</td>
                <td>0.096984</td>
            </tr>
            <tr>
                <td>ash</td>
                <td>0.080741</td>
            </tr>
            <tr>
                <td>water</td>
                <td>-0.183815</td>
            </tr>
            <tr>
                <td>superplastic</td>
                <td>0.237173</td>
            </tr>
            <tr>
                <td>coarseagg</td>
                <td>0.010150</td>
            </tr>
            <tr>
                <td>fineagg</td>
                <td>0.011910</td>
            </tr>
            <tr>
                <td>age</td>
                <td>0.113812</td>
            </tr>
        </tbody>
    </table>

    <h3>Feature Importance</h3>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Weight</th>
                <th>Importance Rank</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>superplastic</td>
                <td>0.2372</td>
                <td>1</td>
            </tr>
            <tr>
                <td>water</td>
                <td>-0.1838</td>
                <td>2</td>
            </tr>
            <tr>
                <td>cement</td>
                <td>0.1141</td>
                <td>3</td>
            </tr>
            <tr>
                <td>age</td>
                <td>0.1138</td>
                <td>4</td>
            </tr>
            <tr>
                <td>slag</td>
                <td>0.0970</td>
                <td>5</td>
            </tr>
            <tr>
                <td>ash</td>
                <td>0.0807</td>
                <td>6</td>
            </tr>
            <tr>
                <td>fineagg</td>
                <td>0.0119</td>
                <td>7</td>
            </tr>
            <tr>
                <td>coarseagg</td>
                <td>0.0102</td>
                <td>8</td>
            </tr>
        </tbody>
    </table>
</section>

<section>
    <h2>Dataset Generation and Comparison of Regularization Methods</h2>
    <p>
        Based on the simulation design explained in class, I generated 200 datasets where the input features exhibit a strong correlation structure (with a correlation coefficient of approximately 0.9). This high degree of correlation was designed to test the robustness of different regularization techniques, specifically ElasticNet, SqrtLasso, and SCAD, in approximating an ideal solution.
    </p>
    <p>
        In this experiment, I designed an ideal "betastar" with a chosen sparsity pattern. The goal was to check which method — ElasticNet, SqrtLasso, or SCAD — produces the best approximation of this "betastar."
    </p>
    <p>
        You will see the following implementations:
    </p>
    <ul>
        <li>
            **SCAD**: The SCAD implementation is already introduced in the <code>SCADLinearModel</code> class shown above. In this section, the SCAD regularization will be tested against the other two methods using the same generated datasets.
        </li>
        <li>
            **ElasticNet**: An <code>ElasticNet</code> class has been implemented, where both the L1 (lasso) and L2 (ridge) penalties are applied to the model to handle correlated features. You will see the implementation of the ElasticNet method and how it performs in approximating the betastar in the generated datasets.
        </li>
        <li>
            **SqrtLasso**: A <code>SqrtLasso</code> class is introduced that employs square-root Lasso regularization. The class includes the necessary adjustments to handle the highly correlated feature space and will be compared to the SCAD and ElasticNet methods.
        </li>
    </ul>

    <p>
        In the following sections, you will find the implementations of each of these regularization techniques, and their corresponding outputs when tested on the generated datasets.
    </p>
</section>
        

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>

</body>
</html>
