<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patrick Church - Assignment 3</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <style>
        body {
            font-family: 'Courier New', Courier, monospace;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
        }
        header {
            background-color: #333;
            color: white;
            padding: 10px;
            text-align: center;
        }
        section {
            margin: 20px;
            padding: 15px;
            background-color: white;
            border-radius: 5px;
            box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);
        }
        h1, h2 {
            color: #333;
        }
        p {
            font-size: 1.0em;
            line-height: 1.6;
            color: #444;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-left: 5px solid #4CAF50;
            overflow-x: auto;
            border-radius: 4px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
            font-size: 0.9em;
        }
        table, th, td {
            border: 1px solid #ccc;
        }
        th, td {
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #333;
            color: white;
        }
    </style>
</head>
<body>

<header>
    <h1>Patrick Church - Assignment 3</h1>
    <p>October 9, 2024</p>
</header>

<section>
    <h2>SCAD Regularization in PyTorch</h2>
    <p>
        I implemented SCAD (smoothly clipped absolute deviation) regularization for linear models in PyTorch. The references for this task are:
    </p>
    <ul>
        <li><a href="https://andrewcharlesjones.github.io/journal/scad.html" target="_blank">SCAD Method Overview</a></li>
        <li><a href="https://www.jstor.org/stable/27640214?seq=1" target="_blank">JSTOR SCAD Regularization</a></li>
    </ul>
    <p>
        Below is the implementation of the SCAD regularization method, tested on the concrete dataset.
    </p>

    <h3>Class Implementation</h3>
    <pre><code class="language-python">
class SCADLinearModel(nn.Module):
    def __init__(self, input_dim, alpha=1.0, a=3.7, device="cpu"):
        super(SCADLinearModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1).double()
        self.alpha = alpha
        self.a = a
        self.device = device
        self.to(device)

    def scad_penalty(self, beta_hat, lambda_val, a_val):
        abs_beta_hat = torch.abs(beta_hat)
        is_linear = abs_beta_hat <= lambda_val
        is_quadratic = (lambda_val < abs_beta_hat) & (abs_beta_hat <= a_val * lambda_val)
        is_constant = abs_beta_hat > a_val * lambda_val
        linear_part = lambda_val * abs_beta_hat * is_linear
        quadratic_part = (2 * a_val * lambda_val * abs_beta_hat - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic
        constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant
        return linear_part + quadratic_part + constant_part

    def forward(self, x):
        return self.linear(x)

    def compute_loss(self, predictions, targets):
        mse_loss = nn.functional.mse_loss(predictions, targets)
        scad_penalty = self.scad_penalty(self.linear.weight, self.alpha, self.a)
        return mse_loss + scad_penalty.sum()


# Testing on the concrete dataset
x = data.drop(columns = ['strength'])
y = data['strength']

x_train_np = x.to_numpy()
y_train_np = y.to_numpy()

x_train = torch.tensor(x_train_np, dtype=torch.double)
y_train = torch.tensor(y_train_np.reshape(-1, 1), dtype=torch.double)

model = SCADLinearModel(input_dim=x_train.shape[1], alpha=1.0, a=3.5, device="cpu")
    
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
        
    predictions = model(x_train)
        
    loss = model.compute_loss(predictions, y_train)
        
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
        
    if (epoch + 1) % 100 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
    
with torch.no_grad():
    weights = model.linear.weight.cpu().numpy()

feature_names = x.columns

feature_importance = list(zip(feature_names, weights[0]))  

print("\nAll Coefficients (Weights):")
print("--------------------------------")
for feature, weight in feature_importance:
    print(f"{feature:<14} : {weight:.6f}")
print("--------------------------------")

feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)

print("\nFeature Importance (sorted by absolute weight value):")
print("-----------------------------------------------------")
print("| Feature        |    Weight    |   Importance Rank  |")
print("-----------------------------------------------------")
for rank, (feature, weight) in enumerate(feature_importance, 1):
    print(f"| {feature:<14} |  {weight:>9.4f}   |        {rank:<2}          |")
print("-----------------------------------------------------")
    </code></pre>

    <h3>Feature Coefficients</h3>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Weight</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>cement</td>
                <td>0.114060</td>
            </tr>
            <tr>
                <td>slag</td>
                <td>0.096984</td>
            </tr>
            <tr>
                <td>ash</td>
                <td>0.080741</td>
            </tr>
            <tr>
                <td>water</td>
                <td>-0.183815</td>
            </tr>
            <tr>
                <td>superplastic</td>
                <td>0.237173</td>
            </tr>
            <tr>
                <td>coarseagg</td>
                <td>0.010150</td>
            </tr>
            <tr>
                <td>fineagg</td>
                <td>0.011910</td>
            </tr>
            <tr>
                <td>age</td>
                <td>0.113812</td>
            </tr>
        </tbody>
    </table>

    <h3>Feature Importance</h3>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Weight</th>
                <th>Importance Rank</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>superplastic</td>
                <td>0.2372</td>
                <td>1</td>
            </tr>
            <tr>
                <td>water</td>
                <td>-0.1838</td>
                <td>2</td>
            </tr>
            <tr>
                <td>cement</td>
                <td>0.1141</td>
                <td>3</td>
            </tr>
            <tr>
                <td>age</td>
                <td>0.1138</td>
                <td>4</td>
            </tr>
            <tr>
                <td>slag</td>
                <td>0.0970</td>
                <td>5</td>
            </tr>
            <tr>
                <td>ash</td>
                <td>0.0807</td>
                <td>6</td>
            </tr>
            <tr>
                <td>fineagg</td>
                <td>0.0119</td>
                <td>7</td>
            </tr>
            <tr>
                <td>coarseagg</td>
                <td>0.0102</td>
                <td>8</td>
            </tr>
        </tbody>
    </table>
</section>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>

</body>
</html>
