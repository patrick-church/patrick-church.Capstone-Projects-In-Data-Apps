<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patrick Church - Assignment 3</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #e0f7fa;
            margin: 0;
            padding: 0;
        }
        header {
            background-color: #0277bd;
            color: white;
            padding: 20px;
            text-align: center;
        }
        section {
            margin: 20px;
            padding: 20px;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1, h2 {
            color: #01579b;
        }
        p {
            font-size: 1.1em;
            line-height: 1.6;
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-left: 5px solid #0277bd;
            overflow-x: auto;
            border-radius: 4px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        table, th, td {
            border: 1px solid #ccc;
        }
        th, td {
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #0288d1;
            color: white;
        }
        ul {
            color: #0277bd;
        }
    </style>
</head>
<body>

<header>
    <h1>Patrick Church - Assignment 3</h1>
    <p>October 9, 2024</p>
</header>

<section>
    <h2>SCAD Regularization and Variable Selection in PyTorch</h2>
    <p>
        This assignment involved implementing a PyTorch class that applies SCAD (smoothly clipped absolute deviation) regularization and variable selection for linear models. The implementation was based on the following references:
    </p>
    <ul>
        <li><a href="https://andrewcharlesjones.github.io/journal/scad.html" target="_blank">SCAD Method Overview</a></li>
        <li><a href="https://www.jstor.org/stable/27640214?seq=1" target="_blank">JSTOR SCAD Regularization</a></li>
    </ul>
    <p>
        The PyTorch class <code>SCADLinearModel</code> was developed to handle linear regression using SCAD regularization. The code includes a forward pass function, a SCAD penalty computation, and a custom loss function that combines mean squared error (MSE) with the SCAD penalty. Below is an example of the code implementation.
    </p>

    <h3>Class Implementation</h3>
    <pre><code class="language-python">
class SCADLinearModel(nn.Module):
    def __init__(self, input_dim, alpha=1.0, a=3.7, device="cpu"):
        super(SCADLinearModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1).double()
        self.alpha = alpha
        self.a = a
        self.device = device
        self.to(device)

    def scad_penalty(self, beta_hat, lambda_val, a_val):
        abs_beta_hat = torch.abs(beta_hat)
        is_linear = abs_beta_hat <= lambda_val
        is_quadratic = (lambda_val < abs_beta_hat) & (abs_beta_hat <= a_val * lambda_val)
        is_constant = abs_beta_hat > a_val * lambda_val
        linear_part = lambda_val * abs_beta_hat * is_linear
        quadratic_part = (2 * a_val * lambda_val * abs_beta_hat - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic
        constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant
        return linear_part + quadratic_part + constant_part

    def forward(self, x):
        return self.linear(x)

    def compute_loss(self, predictions, targets):
        mse_loss = nn.functional.mse_loss(predictions, targets)
        scad_penalty = self.scad_penalty(self.linear.weight, self.alpha, self.a)
        return mse_loss + scad_penalty.sum()


# Testing on the concrete dataset
x = data.drop(columns = ['strength'])
y = data['strength']

x_train_np = x.to_numpy()
y_train_np = y.to_numpy()

x_train = torch.tensor(x_train_np, dtype=torch.double)
y_train = torch.tensor(y_train_np.reshape(-1, 1), dtype=torch.double)

model = SCADLinearModel(input_dim=x_train.shape[1], alpha=1.0, a=3.5, device="cpu")
    
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
        
    predictions = model(x_train)
        
    loss = model.compute_loss(predictions, y_train)
        
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
        
    if (epoch + 1) % 100 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
    
with torch.no_grad():
    weights = model.linear.weight.cpu().numpy()

feature_names = x.columns

feature_importance = list(zip(feature_names, weights[0]))  

print("\nAll Coefficients (Weights):")
print("--------------------------------")
for feature, weight in feature_importance:
    print(f"{feature:<14} : {weight:.6f}")
print("--------------------------------")

feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)

print("\nFeature Importance (sorted by absolute weight value):")
print("-----------------------------------------------------")
print("| Feature        |    Weight    |   Importance Rank  |")
print("-----------------------------------------------------")
for rank, (feature, weight) in enumerate(feature_importance, 1):
    print(f"| {feature:<14} |  {weight:>9.4f}   |        {rank:<2}          |")
print("-----------------------------------------------------")
    </code></pre>

    <h3>Feature Coefficients</h3>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Weight</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>cement</td>
                <td>0.114060</td>
            </tr>
            <tr>
                <td>slag</td>
                <td>0.096984</td>
            </tr>
            <tr>
                <td>ash</td>
                <td>0.080741</td>
            </tr>
            <tr>
                <td>water</td>
                <td>-0.183815</td>
            </tr>
            <tr>
                <td>superplastic</td>
                <td>0.237173</td>
            </tr>
            <tr>
                <td>coarseagg</td>
                <td>0.010150</td>
            </tr>
            <tr>
                <td>fineagg</td>
                <td>0.011910</td>
            </tr>
            <tr>
                <td>age</td>
                <td>0.113812</td>
            </tr>
        </tbody>
    </table>

    <h3>Feature Importance</h3>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Weight</th>
                <th>Importance Rank</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>superplastic</td>
                <td>0.2372</td>
                <td>1</td>
            </tr>
            <tr>
                <td>water</td>
                <td>-0.1838</td>
                <td>2</td>
            </tr>
            <tr>
                <td>cement</td>
                <td>0.1141</td>
                <td>3</td>
            </tr>
            <tr>
                <td>age</td>
                <td>0.1138</td>
                <td>4</td>
            </tr>
            <tr>
                <td>slag</td>
                <td>0.0970</td>
                <td>5</td>
            </tr>
            <tr>
                <td>ash</td>
                <td>0.0807</td>
                <td>6</td>
            </tr>
            <tr>
                <td>fineagg</td>
                <td>0.0119</td>
                <td>7</td>
            </tr>
            <tr>
                <td>coarseagg</td>
                <td>0.0102</td>
                <td>8</td>
            </tr>
        </tbody>
    </table>
</section>

<section>
    <h2>Dataset Generation and Comparison of Regularization Methods</h2>
    <p>
        Based on the simulation design explained in class, I generated 200 datasets where the input features exhibit a strong correlation structure (with a correlation coefficient of approximately 0.9). This high degree of correlation was designed to test the robustness of different regularization techniques, specifically ElasticNet, SqrtLasso, and SCAD, in approximating an ideal solution.
    </p>
    <p>
        In this experiment, I designed an ideal "betastar" with a chosen sparsity pattern. The goal was to check which method — ElasticNet, SqrtLasso, or SCAD — produces the best approximation of this "betastar."
    </p>
    <p>
        You will see the following implementations:
    </p>
    <ul>
        <li>
            **SCAD**: The SCAD implementation is already introduced in the <code>SCADLinearModel</code> class shown above. In this section, the SCAD regularization will be tested against the other two methods using the same generated datasets.
        </li>
        <li>
            **ElasticNet**: An <code>ElasticNet</code> class has been implemented, where both the L1 (lasso) and L2 (ridge) penalties are applied to the model to handle correlated features. You will see the implementation of the ElasticNet method and how it performs in approximating the betastar in the generated datasets.
        </li>
        <li>
            **SqrtLasso**: A <code>SqrtLasso</code> class is introduced that employs square-root Lasso regularization. The class includes the necessary adjustments to handle the highly correlated feature space and will be compared to the SCAD and ElasticNet methods.
        </li>
    </ul>

    <p>
        In the following sections, you will find the implementations of each of these regularization techniques, and their corresponding outputs when tested on the generated datasets.
    </p>
</section>

<section>
    <h2>Betastar Implementation in the SCAD Class</h2>
    <p>
        In this section, we demonstrate how the SCAD regularization method, implemented in the <code>SCADLinearModel</code> class, approximates a true sparse coefficient vector (<code>betastar</code>) across 200 simulated datasets. The following code shows the process of generating datasets with a strong correlation structure and a sparse true coefficient vector (<code>betastar</code>). We then train the SCAD model on these datasets to estimate the feature importance and compare the estimated coefficients to the true <code>betastar</code>.
    </p>
    
    <h3>Code for Dataset Generation and SCAD Training</h3>
    <pre><code class="language-python">
def mse_coefficients(betastar, estimated_coeffs):
    return np.mean((betastar - estimated_coeffs) ** 2)

def make_correlated_features(num_samples, p, rho, betastar, noise_std=0.1):
    vcor = [rho ** i for i in range(p)]
    r = toeplitz(vcor)
    mu = np.zeros(p)
    X = np.random.multivariate_normal(mu, r, size=num_samples)
    epsilon = np.random.normal(0, noise_std, size=(num_samples, 1))
    y = X @ betastar.reshape(-1, 1) + epsilon  
    return X, y

def create_betastar(p, sparsity_level):
    betastar = np.zeros(p)
    non_zero_indices = np.random.choice(p, sparsity_level, replace=False)
    betastar[non_zero_indices] = np.random.uniform(1, 5, size=sparsity_level)
    return betastar

rho = 0.9
p = 20
n = 150
num_datasets = 200
sparsity_level = 5
noise_std = 0.1

betastar = create_betastar(p, sparsity_level)
print("True betastar:", betastar)

datasets = [make_correlated_features(n, p, rho, betastar, noise_std) for _ in range(num_datasets)]

all_feature_importances = []  
all_losses = []  
mse_results = []  

for i, (X_train, y_train) in enumerate(datasets):
    x_train_tensor = torch.tensor(X_train, dtype=torch.double)
    y_train_tensor = torch.tensor(y_train, dtype=torch.double)

    model = SCADLinearModel(input_dim=p, alpha=1.0, a=3.5, device="cpu")
    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    num_epochs = 1000
    for epoch in range(num_epochs):
        model.train()
        predictions = model(x_train_tensor)
        loss = model.compute_loss(predictions, y_train_tensor)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    all_losses.append(loss.item())

    with torch.no_grad():
        weights = model.linear.weight.cpu().numpy()

    all_feature_importances.append(weights[0])

    mse = mse_coefficients(betastar, weights[0])
    mse_results.append(mse)

avg_feature_importance = np.mean(all_feature_importances, axis=0)

feature_names = [f'Feature {i+1}' for i in range(p)]

average_importance = list(zip(feature_names, avg_feature_importance))

average_importance.sort(key=lambda x: abs(x[1]), reverse=True)

print("Average Feature Importance (sorted by absolute weight value across 200 datasets):")
print("-----------------------------------------------------")
print("| Feature        |    Avg Weight   |   Importance Rank  |")
print("-----------------------------------------------------")
for rank, (feature, weight) in enumerate(average_importance, 1):
    print(f"| {feature:<14} |  {weight:>9.4f}     |        {rank:<2}          |")
print("-----------------------------------------------------")

avg_loss = np.mean(all_losses)
print(f"\nAverage Loss across 200 datasets: {avg_loss:.4f}")

avg_mse = np.mean(mse_results)
print(f"Average MSE between SCAD-estimated coefficients and betastar across 200 datasets: {avg_mse:.4f}")
    </code></pre>

    <h3>Explanation</h3>
    <p>
        This code performs the following:
    </p>
    <ul>
        <li><strong>MSE Calculation:</strong> The <code>mse_coefficients</code> function computes the mean squared error (MSE) between the true <code>betastar</code> and the estimated coefficients from the SCAD model. This measures how close the model’s estimates are to the true underlying values.</li>
        <li><strong>Generating Correlated Datasets:</strong> The <code>make_correlated_features</code> function generates datasets with a strong correlation structure between features (correlation of ~0.9) and adds some noise to simulate real-world data.</li>
        <li><strong>Creating Betastar:</strong> The <code>create_betastar</code> function creates a sparse coefficient vector, where only a few of the features are assigned non-zero values. This is the true signal that the SCAD model will attempt to recover.</li>
        <li><strong>Training the SCAD Model:</strong> The code then trains the SCAD model on each dataset for 1000 epochs, and after training, the estimated coefficients are compared to the true <code>betastar</code>.</li>
    </ul>

    <h3>Results</h3>
    <p>Here are the results from running the SCAD model on 200 datasets:</p>

    <h4>True Betastar</h4>
    <pre><code>True betastar: [0., 0., 0., 0., 0., 0., 0., 3.8197, 4.2420, 3.1721, 0., 0., 3.0342, 0., 4.6579, 0., 0., 0., 0., 0.]</code></pre>

    <h4>Average Feature Importance (sorted by absolute weight value across 200 datasets)</h4>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Average Weight</th>
                <th>Importance Rank</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Feature 15</td><td>5.5484</td><td>1</td></tr>
            <tr><td>Feature 9</td><td>4.7952</td><td>2</td></tr>
            <tr><td>Feature 8</td><td>3.8178</td><td>3</td></tr>
            <tr><td>Feature 10</td><td>3.0063</td><td>4</td></tr>
            <tr><td>Feature 13</td><td>1.6533</td><td>5</td></tr>
            <tr><td>Feature 12</td><td>0.0088</td><td>6</td></tr>
            <tr><td>Feature 14</td><td>0.0063</td><td>7</td></tr>
            <tr><td>Feature 11</td><td>0.0053</td><td>8</td></tr>
            <tr><td>Feature 19</td><td>0.0019</td><td>9</td></tr>
            <tr><td>Feature 1</td><td>0.0017</td><td>10</td></tr>
            <tr><td>Feature 4</td><td>0.0011</td><td>11</td></tr>
            <tr><td>Feature 3</td><td>0.0010</td><td>12</td></tr>
            <tr><td>Feature 5</td><td>0.0009</td><td>13</td></tr>
            <tr><td>Feature 7</td><td>0.0009</td><td>14</td></tr>
            <tr><td>Feature 20</td><td>-0.0008</td><td>15</td></tr>
            <tr><td>Feature 18</td><td>-0.0007</td><td>16</td></tr>
            <tr><td>Feature 16</td><td>-0.0006</td><td>17</td></tr>
            <tr><td>Feature 2</td><td>0.0003</td><td>18</td></tr>
            <tr><td>Feature 17</td><td>0.0001</td><td>19</td></tr>
            <tr><td>Feature 6</td><td>0.0001</td><td>20</td></tr>
        </tbody>
    </table>

    <h4>Other Results</h4>
    <p>
        <strong>Average Loss:</strong> The average loss across the 200 datasets was <strong>11.0002</strong>. This value includes both the MSE and the SCAD penalty, showing how well the model fits the data while regularizing the coefficients.<br>
        <strong>Average MSE:</strong> The average mean squared error between the SCAD-estimated coefficients and the true <code>betastar</code> was <strong>0.3891</strong>, indicating that the SCAD model was able to approximate the true coefficients quite well.
    </p>
</section>


<section>
    <h2>Betastar Implementation in the ElasticNet Class</h2>
    <p>
        In this section, we describe the implementation of ElasticNet regularization using the <code>ElasticNet</code> class. ElasticNet is a combination of L1 (Lasso) and L2 (Ridge) regularization, controlled by the <code>l1_ratio</code> parameter, making it suitable for handling datasets with multicollinearity. The code below generates correlated datasets and trains the ElasticNet model to approximate the true <code>betastar</code> values across 200 datasets.
    </p>

    <h3>Code for ElasticNet Implementation and Dataset Generation</h3>
    <pre><code class="language-python">
class ElasticNet(nn.Module):
    def __init__(self, input_size, alpha=1.0, l1_ratio=0.5):
        super(ElasticNet, self).__init__()
        self.input_size = input_size
        self.alpha = alpha
        self.l1_ratio = l1_ratio

        self.linear = nn.Linear(input_size, 1).double()

    def forward(self, x):
        return self.linear(x)

    def loss(self, y_pred, y_true):
        mse_loss = nn.MSELoss()(y_pred, y_true)
        l1_reg = torch.norm(self.linear.weight, p=1)
        l2_reg = torch.norm(self.linear.weight, p=2)

        loss = mse_loss + self.alpha * (
            self.l1_ratio * l1_reg + (1 - self.l1_ratio) * l2_reg
        )

        return loss

    def fit(self, X, y, num_epochs=1000, learning_rate=0.01):
        optimizer = optim.SGD(self.parameters(), lr=learning_rate)

        for epoch in range(num_epochs):
            self.train()
            optimizer.zero_grad()
            y_pred = self(X)
            loss = self.loss(y_pred, y)
            loss.backward()
            optimizer.step()

    def predict(self, X):
        self.eval()
        with torch.no_grad():
            y_pred = self(X)
        return y_pred

    def get_coefficients(self):
        return self.linear.weight

def mse_coefficients(betastar, estimated_coeffs):
    return np.mean((betastar - estimated_coeffs) ** 2)

def make_correlated_features(num_samples, p, rho, betastar, noise_std=0.1):
    vcor = [rho ** i for i in range(p)]
    r = toeplitz(vcor)
    mu = np.zeros(p)
    X = np.random.multivariate_normal(mu, r, size=num_samples)
    epsilon = np.random.normal(0, noise_std, size=(num_samples, 1))
    y = X @ betastar.reshape(-1, 1) + epsilon  
    return X, y

def create_betastar(p, sparsity_level):
    betastar = np.zeros(p)
    non_zero_indices = np.random.choice(p, sparsity_level, replace=False)
    betastar[non_zero_indices] = np.random.uniform(1, 5, size=sparsity_level)
    return betastar

rho = 0.9
p = 20
n = 150
num_datasets = 200
sparsity_level = 5
noise_std = 0.1

betastar = create_betastar(p, sparsity_level)
print("True betastar:", betastar)

datasets = [make_correlated_features(n, p, rho, betastar, noise_std) for _ in range(num_datasets)]

all_feature_importances = [] 
all_losses = []  
mse_results = []  

for i, (X_train, y_train) in enumerate(datasets):
    
    x_train_tensor = torch.tensor(X_train, dtype=torch.double)
    y_train_tensor = torch.tensor(y_train, dtype=torch.double)

    
    model = ElasticNet(input_size=p, alpha=1.0, l1_ratio=0.5)  
    model.fit(x_train_tensor, y_train_tensor, num_epochs=1000, learning_rate=0.01)

    with torch.no_grad():
        estimated_coeffs = model.get_coefficients().cpu().numpy().flatten()

    all_feature_importances.append(estimated_coeffs)

    mse = mse_coefficients(betastar, estimated_coeffs)
    mse_results.append(mse)

    final_loss = model.loss(model.predict(x_train_tensor), y_train_tensor).item()
    all_losses.append(final_loss)

avg_feature_importance = np.mean(all_feature_importances, axis=0)

feature_names = [f'Feature {i+1}' for i in range(p)]

average_importance = list(zip(feature_names, avg_feature_importance))

average_importance.sort(key=lambda x: abs(x[1]), reverse=True)

print("Average Feature Importance (sorted by absolute weight value across 200 datasets):")
print("-----------------------------------------------------")
print("| Feature        |    Avg Weight   |   Importance Rank  |")
print("-----------------------------------------------------")
for rank, (feature, weight) in enumerate(average_importance, 1):
    print(f"| {feature:<14} |  {weight:>9.4f}     |        {rank:<2}          |")
print("-----------------------------------------------------")

avg_loss = np.mean(all_losses)
print(f"\nAverage Loss across 200 datasets: {avg_loss:.4f}")

avg_mse = np.mean(mse_results)
print(f"Average MSE between ElasticNet-estimated coefficients and betastar across 200 datasets: {avg_mse:.4f}")
    </code></pre>

    <h3>Explanation</h3>
    <p>
        The <code>ElasticNet</code> class is designed to combine both L1 (Lasso) and L2 (Ridge) regularization terms. By adjusting the <code>l1_ratio</code> parameter, we can control the contribution of L1 vs L2 regularization. This method is particularly useful for handling datasets with multicollinearity.
    </p>
    <ul>
        <li><strong>ElasticNet Loss Function:</strong> The loss function combines MSE with a penalty that is a weighted sum of the L1 and L2 norms of the coefficients. This helps regularize the model and prevent overfitting, especially in datasets with correlated features.</li>
        <li><strong>Dataset Generation:</strong> Similar to the SCAD implementation, we generate 200 datasets with a strong correlation structure and apply the ElasticNet model to estimate the coefficients.</li>
        <li><strong>Betastar:</strong> The true coefficients vector <code>betastar</code> is generated with sparsity, which the ElasticNet model attempts to recover.</li>
    </ul>

    <h3>Results</h3>
    <p>Here are the results from running the ElasticNet model on 200 datasets:</p>

    <h4>True Betastar</h4>
    <pre><code>True betastar: [3.2384, 0., 0., 0., 1.3742, 0., 0., 3.4213, 0., 0., 0., 0., 0., 0., 1.3335, 0., 1.5703, 0., 0., 0.]</code></pre>

    <h4>Average Feature Importance (sorted by absolute weight value across 200 datasets)</h4>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Average Weight</th>
                <th>Importance Rank</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Feature 1</td><td>2.3248</td><td>1</td></tr>
            <tr><td>Feature 8</td><td>2.1266</td><td>2</td></tr>
            <tr><td>Feature 17</td><td>0.9840</td><td>3</td></tr>
            <tr><td>Feature 15</td><td>0.8481</td><td>4</td></tr>
            <tr><td>Feature 5</td><td>0.8416</td><td>5</td></tr>
            <tr><td>Feature 7</td><td>0.5500</td><td>6</td></tr>
            <tr><td>Feature 2</td><td>0.5407</td><td>7</td></tr>
            <tr><td>Feature 9</td><td>0.4709</td><td>8</td></tr>
            <tr><td>Feature 16</td><td>0.4080</td><td>9</td></tr>
            <tr><td>Feature 6</td><td>0.2967</td><td>10</td></tr>
            <tr><td>Feature 4</td><td>0.2207</td><td>11</td></tr>
            <tr><td>Feature 14</td><td>0.1806</td><td>12</td></tr>
            <tr><td>Feature 18</td><td>0.1596</td><td>13</td></tr>
            <tr><td>Feature 3</td><td>0.1358</td><td>14</td></tr>
            <tr><td>Feature 10</td><td>0.0849</td><td>15</td></tr>
            <tr><td>Feature 13</td><td>0.0424</td><td>16</td></tr>
            <tr><td>Feature 12</td><td>0.0292</td><td>17</td></tr>
            <tr><td>Feature 11</td><td>0.0272</td><td>18</td></tr>
            <tr><td>Feature 19</td><td>0.0177</td><td>19</td></tr>
            <tr><td>Feature 20</td><td>0.0081</td><td>20</td></tr>
        </tbody>
    </table>

    <h4>Other Results</h4>
    <p>
        <strong>Average Loss:</strong> The average loss across the 200 datasets was <strong>7.5427</strong>, indicating how well the model fits the data while regularizing the coefficients.<br>
        <strong>Average MSE:</strong> The average mean squared error between the ElasticNet-estimated coefficients and the true <code>betastar</code> was <strong>0.2379</strong>, showing that the ElasticNet model was effective in approximating the true coefficients.
    </p>
</section>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>

</body>
</html>
