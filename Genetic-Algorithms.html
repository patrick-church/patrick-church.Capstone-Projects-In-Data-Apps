<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Genetic Algorithm Implementation</title>
    
    <!-- Load Prism.js CSS for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.css" rel="stylesheet" />

    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        h1 {
            color: #2c3e50;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            font-family: Consolas, monospace;
        }
        .section-title {
            color: #2980b9;
        }
    </style>
</head>
<body>

    <h1>Genetic Algorithm Implementation</h1>

    <p>This webpage demonstrates how to use a Genetic Algorithm (GA) to optimize hyperparameters for a Random Forest model.</p>

    <h2 class="section-title">Full Python Code</h2>
    
    <pre><code class="language-python">
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import random

# 1. Split the dataset
X = data.drop(columns='strength')  # Features
y = data['strength']  # Target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 2. Fitness function: Train model and calculate error
def evaluate(hyperparameters):
    n_estimators, max_depth, min_samples_split = hyperparameters
    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    return mean_squared_error(y_test, predictions)

# 3. Generate random hyperparameters
def create_individual():
    return [random.randint(10, 100),  # Number of trees
            random.randint(2, 10),    # Max depth of trees
            random.uniform(0.1, 1.0)] # Min samples to split

# 4. Create a simple population
population = [create_individual() for _ in range(10)]

# 5. Evaluate each set of hyperparameters
for individual in population:
    error = evaluate(individual)
    print(f"Hyperparameters: {individual}, Error: {error}")
    </code></pre>

    <h2 class="section-title">Conclusion</h2>
    <p>This simplified version demonstrates the basic steps of using a genetic algorithm to tune hyperparameters, including creating random hyperparameter sets, training a model, and measuring error.</p>

    <!-- Load Prism.js for Python code syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>

</body>
</html>
