<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Genetic Algorithm Implementation</title>
    
    <!-- Load Prism.js CSS for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.css" rel="stylesheet" />

    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        h1 {
            color: #2c3e50;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            font-family: Consolas, monospace;
        }
        .section-title {
            color: #2980b9;
        }
    </style>
</head>
<body>

    <h1>Genetic Algorithm Implementation</h1>

    <p>This webpage demonstrates how to use a Genetic Algorithm (GA) to optimize hyperparameters for a Random Forest model.</p>

    <h2 class="section-title">Full Python Code</h2>
    
    <pre><code class="language-python">
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from deap import base, creator, tools, algorithms
import random

# Split the data into features (X) and target (y)
X = data.drop(columns='strength')
y = data['strength']

# Step 1: Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Define the fitness function (minimize error)
# We're trying to minimize the model's prediction error (mean squared error)
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

# Step 3: Initialize population with random hyperparameters
# Each individual represents a combination of random hyperparameters
def create_individual():
    return [random.randint(10, 200),      # Random value for n_estimators (number of trees)
            random.randint(2, 20),        # Random value for max_depth (tree depth)
            random.uniform(0.1, 1.0)]     # Random value for min_samples_split (min data to split)

toolbox = base.Toolbox()
toolbox.register("individual", tools.initIterate, creator.Individual, create_individual)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Step 4: Evaluate each set of hyperparameters by training the model
def evaluate(individual):
    # Get the hyperparameters from the individual
    n_estimators = individual[0]
    max_depth = individual[1]
    min_samples_split = individual[2]
    
    # Create a RandomForest model using the current hyperparameters
    model = RandomForestRegressor(n_estimators=n_estimators, 
                                  max_depth=max_depth, 
                                  min_samples_split=min_samples_split)
    model.fit(X_train, y_train)  # Train the model
    
    # Test the model and calculate the error (MSE)
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    
    # Return the error (lower is better)
    return mse,

toolbox.register("evaluate", evaluate)

# Step 5: Crossover - combine parts of two sets of hyperparameters to create a new one
toolbox.register("mate", tools.cxBlend, alpha=0.5)

# Step 6: Mutation - randomly tweak some hyperparameters to explore new options
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)

# Step 7: Selection - choose the best sets of hyperparameters for the next generation
toolbox.register("select", tools.selTournament, tournsize=3)

# Step 8: Run the genetic algorithm for 40 generations, evolving 50 individuals
population = toolbox.population(n=50)
algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=40, verbose=True)

# After evolution, retrieve the best hyperparameters found
best_individual = tools.selBest(population, k=1)[0]
print(f"Best hyperparameters: n_estimators={best_individual[0]}, max_depth={best_individual[1]}, min_samples_split={best_individual[2]}")
    </code></pre>

    <h2 class="section-title">Conclusion</h2>
    <p>This implementation of a genetic algorithm optimizes hyperparameters for a Random Forest model, evolving the population over time to find the best solution.</p>

    <!-- Load Prism.js for Python code syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>

</body>
</html>
