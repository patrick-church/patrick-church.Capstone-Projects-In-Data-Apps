<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Boosting in Locally Weighted Regression</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/components/prism-python.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        h1, h2 {
            color: #2E86C1;
        }
        p {
            font-size: 1.1em;
        }
        .header {
            background-color: #f4f4f4;
            padding: 20px;
            border-radius: 10px;
        }
        .section {
            margin-top: 20px;
        }
        pre {
            background: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
        }
    </style>
</head>
<body>

    <div class="header">
        <h1>Patrick Church</h1>
        <p>Class: Data 440</p>
        <p>Date: 9/26/24</p>
    </div>

    <div class="section">
        <h2>Gradient Boosting in Locally Weighted Regression</h2>
    </div>

    <div class="section">
        <h2>Patricks_Gradient_Boosted_LWR Class</h2>
        <p>
            The <strong>Patricks_Gradient_Boosted_LWR</strong> class is designed to implement gradient boosting in the context of Locally Weighted Regression (LWR). 
            This class introduces flexibility and customization for users by allowing them to specify the number of boosting iterations they would like to perform when 
            running the <code>cross_validate</code> function. Additionally, it provides an option for users to choose between three different standardization techniques: 
            <strong>QuantileScaler</strong>, <strong>MinMaxScaler</strong>, and <strong>StandardScaler</strong>. These scalers are key to ensuring that data is appropriately 
            scaled before running the regression models.
        </p>

        <h2>Key Features:</h2>
        <ul>
            <li><strong>User-Controlled Gradient Boosting:</strong>
                The user can control the number of boosting stages using the <code>n_boosts</code> argument in the <code>cross_validate</code> function. This flexibility 
                allows the user to customize how much the model iteratively refines the residuals through multiple boosting steps.
            </li>

            <li><strong>Choice of Standardization Method:</strong>
                The <code>cross_validate</code> function allows the user to choose between three different data standardization methods:
                <ul>
                    <li><strong>StandardScaler:</strong> Standardizes features by removing the mean and scaling to unit variance.</li>
                    <li><strong>MinMaxScaler:</strong> Scales features to a given range (usually between 0 and 1).</li>
                    <li><strong>QuantileScaler:</strong> Transforms features to follow a uniform or normal distribution.</li>
                </ul>
                The comparison between these scalers is a core feature of the class, as it shows how different data transformations impact model performance.
            </li>

            <li><strong>Cross-Validation and Model Comparison:</strong>
                The class uses <strong>10-fold cross-validation</strong> to train and test the locally weighted regression model. It compares the performance of the 
                <strong>Locally Weighted Regression</strong> model against the <strong>eXtreme Gradient Boosting (XGBoost)</strong> model. The cross-validation function computes the 
                <strong>Mean Squared Error (MSE)</strong> for both models and prints the results for direct comparison.
            </li>

            <li><strong>Competing with XGBoost:</strong>
                One of the primary goals of this class is to demonstrate that <strong>Locally Weighted Regression</strong> (when enhanced with gradient boosting) 
                can compete with the widely used <strong>XGBoost</strong> model. By refining the predictions through boosting and scaling the data appropriately, 
                the class shows how LWR can achieve comparable or better MSEs than XGBoost in regression tasks.
            </li>
        </ul>
    </div>

    <div class="section">
        <h2>Python Class: Patricks_Gradient_Boosted_LWR</h2>
        <pre><code class="language-python">
            
class Patricks_Gradient_Boosted_LWR:
    def __init__(self, kernel=Gaussian, tau=4.0):
        self.kernel = kernel
        self.tau = tau
        self._is_fitted = False  # A flag to track if the model is fitted

    def fit(self, x, y):
        self.xtrain_ = x
        self.yhat_ = y
        self._is_fitted = True  # Set the flag to True after fitting the model

    def is_fitted(self):
        if not self._is_fitted:
            raise ValueError("This Lowess_2 instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.")

    def predict(self, x_new):
        # Check if the model is fitted
        self.is_fitted()

        x = self.xtrain_
        y = self.yhat_
        lm = Ridge(alpha=0.001)
        w = weight_function(x, x_new, self.kernel, self.tau)

        if np.isscalar(x_new):
            lm.fit(np.diag(w) @ (x.reshape(-1, 1)), np.diag(w) @ (y.reshape(-1, 1)))
            yest = lm.predict([[x_new]])[0][0]
        else:
            n = len(x_new)
            yest_test = []
            for i in range(n):
                lm.fit(np.diag(w[:, i]) @ x, np.diag(w[:, i]) @ y)
                yest_test.append(lm.predict(x_new[i].reshape(1, -1)))
        return np.array(yest_test).reshape(-1, 1)

    def boosted_lwr_more(self, x, y, xnew, n_boosts=3):
        model1 = Patricks_Gradient_Boosted_LWR(kernel=Gaussian, tau=0.35)
        model1.fit(x, y)
        predictions = model1.predict(xnew)

        residuals = y - model1.predict(x).ravel()

        for i in range(1, n_boosts):
            if i % 2 == 0:
                model = Patricks_Gradient_Boosted_LWR(kernel=Gaussian, tau=0.35)
            else:
                model = Patricks_Gradient_Boosted_LWR(kernel=Tricubic, tau=0.23)

            model.fit(x, residuals)
            predictions += model.predict(xnew)
            residuals -= model.predict(x).ravel()

        return predictions

    def cross_validate(self, X, y, scaling_method='standard', kfold_splits=10, seed=42, boost_rounds=3):

        if scaling_method == 'minmax':
            scaler = MinMaxScaler()
        elif scaling_method == 'quantile':
            scaler = QuantileTransformer(n_quantiles=900)
        else:
            scaler = StandardScaler()

        mse_lwr_results = []
        mse_xgb_results = []

        kfold = KFold(n_splits=kfold_splits, shuffle=True, random_state=seed)
        xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, reg_lambda=20, alpha=1, gamma=10, max_depth=7)

        for train_idx, test_idx in kfold.split(X):
            X_train = X[train_idx]
            y_train = y[train_idx]
            y_test = y[test_idx]
            X_test = X[test_idx]

            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

            y_pred_lwr = self.boosted_lwr_more(X_train, y_train, X_test, n_boosts=boost_rounds)

            xgb_model.fit(X_train, y_train)
            y_pred_xgb = xgb_model.predict(X_test)

            mse_lwr_results.append(mean_squared_error(y_test, y_pred_lwr))
            mse_xgb_results.append(mean_squared_error(y_test, y_pred_xgb))

        print(f'The Cross-validated Mean Squared Error for Locally Weighted Regression is: {np.mean(mse_lwr_results)}')
        print(f'The Cross-validated Mean Squared Error for XGBRegressor is: {np.mean(mse_xgb_results)}')
        </code></pre>
    </div>

    <div class="section">
        <h2>Function Summaries</h2>
        <ul>
            <li><strong>__init__(self, kernel=Gaussian, tau=0.03):</strong>
                This is the constructor that initializes the class. It sets the kernel (defaulting to Gaussian) and the tau parameter (a bandwidth parameter controlling the weight function). It also sets a flag, _is_fitted, to False, indicating that the model has not yet been trained.
            </li>
            <li><strong>fit(self, x, y):</strong>
                This method trains the model by storing the training data (x for features, y for target values) and setting the _is_fitted flag to True. It does not perform any actual fitting, but prepares the model with the data for later predictions.
            </li>
            <li><strong>is_fitted(self):</strong>
                This is a helper function that checks if the model has been fitted (trained). If the model has not been fitted, it raises a ValueError, ensuring that predictions are only made after the model has been trained.
            </li>
            <li><strong>predict(self, x_new):</strong>
                This method predicts the target values for new data (x_new) using the fitted model. It checks if the model is fitted using is_fitted(), computes the weights using a kernel function, and then applies ridge regression to make the predictions. If the input is scalar, it predicts a single value; otherwise, it predicts for multiple data points.
            </li>
            <li><strong>boosted_lwr_more(self, x, y, xnew, n_boosts=3):</strong>
                This function implements gradient boosting for locally weighted regression. It trains the model (Patricks_Gradient_Boosted_LWR) on the residuals in multiple boosting rounds (controlled by the n_boosts argument). Each boosting step fits a new model on the residual errors from the previous prediction and updates the overall prediction.
            </li>
            <li><strong>cross_validate(self, X, y, scaling_method='standard', kfold_splits=10, seed=42, boost_rounds=3):</strong>
                This function performs 10-fold cross-validation on the dataset, comparing the locally weighted regression model against the XGBoost model. It allows the user to choose between different scaling methods (StandardScaler, MinMaxScaler, QuantileScaler) and specify the number of boosting rounds (boost_rounds). The function computes the Mean Squared Error (MSE) for both models and prints the results.
            </li>
        </ul>
    </div>



</body>
</html>
